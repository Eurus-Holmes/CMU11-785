{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"350px\" src=\"files/Gan_Arch.png\">\n",
    "\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "<img width=\"500px\" src=\"files/loss.png\">\n",
    "\n",
    "*** \n",
    "#### Discriminator Objective \n",
    "\n",
    "Maximize the chance to recognize real images as real and generated images as fake.\n",
    "\n",
    "#### Generator Objective \n",
    "\n",
    "Generate images with the highest possible value of D(x) to fool the discriminator.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the loss function defined, let's get into training GANs.\n",
    "\n",
    "Our main focus here is to generate data from scratch i.e from a noise vector. \n",
    "\n",
    "The generator upsamples the data using a **transposed convolution** operation (a.k.a deconvolution a.k.a fractionally-strided convolution)\n",
    "\n",
    "<table style=\"width:100%; table-layout:fixed;\">\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"files/no_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"files/no_padding_no_strides_transposed.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Convolution (Downsampling)</td>\n",
    "    <td>Transposed Convolution (Upsampling)</td>\n",
    "  </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "An example script for creating and running a GAN is in the repository `mnist_gan.py`.\n",
    "\n",
    "- Inferno is used for training and logging\n",
    "- The main inferno training loop pumps real images and trains the discriminator\n",
    "- A callback periodically trains the generator\n",
    "\n",
    "If you want the full experience, please try running tensorboard while the script runs. \n",
    "\n",
    "- Live images will be drawn to the webpage as your network trains.\n",
    "- A video will be rendered when training completes (if you install ffmpeg).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorboard](https://github.com/cmudeeplearning11785/deep-learning-tutorials/raw/master/recitation-10/images/tensorboard1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alternatively, there is also a sample pytorch implementation of the DCGAN on mnist in the repository [dcgan_mnist_pytorch.ipynb]\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the actual code from the linked file\n",
    "import mnist_gan\n",
    "import mnist_wgangp\n",
    "import mnist_cwgangp\n",
    "import cifar10_wgangp\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Successful GAN\n",
    "\n",
    "First we train the GAN with settings that converge (found through trial-and-error). The generator and discriminator both have a learning rate of 3e-4 and the generator is trained 1 time every 5 times the discriminator is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_gan.main([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/embed/IUi0REAWj2c?rel=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failed GAN\n",
    "\n",
    "Here we see what happens if the generator is trained too much or too little compared to the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_gan.main(['--generator-frequency=1', '--save-directory=output/mnist_gan/frequency-1', '--epochs=50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/embed/J8m1NXLwSKw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## GANs can be HARD to train. \n",
    "\n",
    "- The parameters oscillate, destabilize and never converge. Unbalance between generator and discriminator.\n",
    "\n",
    "- Mode collapse - the generator collapses which produces limited varieties of samples\n",
    "\n",
    "<table style=\"width:100%; table-layout:fixed;\">\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"images/mode.png\"></td>\n",
    "    <td><img width=\"150px\" src=\"images/mode1.png\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "</table>\n",
    "\n",
    "- Diminished gradient: the discriminator ends up dominating, and the generator gradients start to vanish and it learns nothing\n",
    "\n",
    "- Highly sensitive to the hyperparameter selections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to improve GAN Performance\n",
    "\n",
    "- **Feature matching** \n",
    "\n",
    "Minimize the statistical difference between the features of the real images and the generated images by computing the L2-distance between the means of their feature vectors. Add this L2 distance to your generator loss. Feature matching expands the goal from beating the opponent to matching features in real images. \n",
    "\n",
    "- **Minibatch discrimination**\n",
    "\n",
    "When mode collapses, all images created looks similar. To mitigate the problem, we feed real images and generated images into the discriminator separately in different batches and compute the similarity of the image x with images in the same batch. The discriminator can use this score to detect generated images and penalize the generator if mode is collapsing.\n",
    "\n",
    "- **One sided label smoothing**\n",
    "\n",
    "Deep networks may suffer from overconfidence. For example, it uses very few features to classify an object. To mitigate the problem, deep learning uses regulation and dropout to avoid overconfidence. To avoid the problem, we penalize the discriminator when the prediction for any real images go beyond 0.9 (D(real image)>0.9). This is done by setting our target label value to be 0.9 instead of 1.0. \n",
    "\n",
    "- **Changing the cost functions**\n",
    "\n",
    "*WGAN-GP, LSGAN, BEGAN, DRAGAN*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein GAN with Gradient Penalty\n",
    "\n",
    "Here we see an improvement on traditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_wgangp.main([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=unXILX2wp1A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN-GP on CIFAR10\n",
    "\n",
    "For a slightly more complicated dataset, this example uses CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_wgangp.main([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/dAe-UcOfywE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional WGAN-GP on MNIST\n",
    "\n",
    "Here use use a conditional GAN to learn each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_cwgangp.main([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/_wuRRwujeHc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to evaluate different GAN models?\n",
    "\n",
    "- **Inception score** \n",
    "\n",
    "Measures the performance of the GAN based on quality of generated images and their diversity. High inception score is good.\n",
    "\n",
    "- **Fr√©chet Inception Distance (FID)**\n",
    "\n",
    "In FID, we use the Inception network to extract features from an intermediate layer. Then we model the data distribution for these features using a multivariate Gaussian distribution.\n",
    "The FID between the real images x and generated images g is computed as : (Tr sums up all the diagonal elements).\n",
    "Low FID score is good\n",
    "\n",
    "- **Precision, Recall, F1 score**\n",
    "\n",
    "If the generated images look similar to the real images on average, the precision is high. High recall implies the generator can generate any sample found in the training dataset. A F1 score is the harmonic average of precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-------- Latest Work in GANs -----------------\n",
    "\n",
    "--------- TODO --------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
