\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Deep learning Theory}{3}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}MLP: Intuitive Understanding}{3}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}MLP: Theoretical Proof}{6}{subsection.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Networks: Part 1}{14}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Basic Concepts And Theoretical Foundations}{14}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gradient Descent Theory and Objective Function}{21}{subsection.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neural Networks: Part 2}{24}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Backward Propagation Theory And Convergence}{24}{subsection.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Neural Network Training}{28}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Concept combing}{29}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Three ways to reduce the gradient}{30}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Several methods of optimizing the algorithm:}{32}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Generalization strategy}{33}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Training skills}{35}{subsection.4.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Divergence Evaluation Function}{35}{subsubsection.4.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Batch Normalization}{36}{subsubsection.4.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Other Tips}{37}{subsubsection.4.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Convolutional Nerual Networks (CNN)}{38}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Different layers in CNN}{38}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Convolution Layer}{38}{subsubsection.5.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Pooling Layer}{39}{subsubsection.5.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Multi-class learning}{39}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Derivation}{39}{subsubsection.5.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Notes}{39}{subsubsection.5.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Computation Improvment}{40}{subsection.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}CNN Case studies}{40}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Classic Networks}{40}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}LeNet-5}{40}{subsubsection.6.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}AlexNet}{40}{subsubsection.6.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}VGG-16}{40}{subsubsection.6.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}ResNet}{41}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Introduction}{41}{subsubsection.6.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Intuition}{41}{subsubsection.6.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}1*1 convolution}{41}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Inception Network}{41}{subsection.6.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Pratical Advices}{41}{subsection.6.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.1}Data Augmentation}{41}{subsubsection.6.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2}Deep Learning for Computer Vision}{42}{subsubsection.6.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Detection Algorithm}{42}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Object Location}{42}{subsection.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Convolutional Implementation of Sliding Windows}{42}{subsection.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}YOLO Algorithm}{42}{subsection.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Intersection Over Union}{43}{subsection.7.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Non-max Suppression}{43}{subsection.7.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Anchor Boxes}{43}{subsection.7.6}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Special Applications}{43}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Face Recognition}{43}{subsection.8.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Face verification}{43}{subsubsection.8.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Nerual Network For Degree Difference - Siamese Network}{43}{subsubsection.8.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}Triplet Loss}{43}{subsubsection.8.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.4}Binary Classification}{44}{subsubsection.8.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Neural Style Transfer}{44}{subsection.8.2}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Recurrent Neural Networks (RNN)}{44}{section.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces RNN}}{44}{figure.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RNN examples}}{45}{figure.2}}
\newlabel{fig:rnn_examples}{{2}{45}{RNN examples}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Stacked RNNs}}{45}{figure.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Practical Example}{45}{subsection.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Image Captioning}{45}{subsection.9.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Practical example. You can see this blue boxes being softmax classifier, in other words, at each time step there is a softmax classifier.}}{46}{figure.4}}
\newlabel{fig:RNN_toghether}{{4}{46}{Practical example. You can see this blue boxes being softmax classifier, in other words, at each time step there is a softmax classifier}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces "Deep Visual-Semantic Alignments for Generating Image Descriptions", Karpathy and Fei-Fei}}{46}{figure.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}LSTM}{46}{subsection.9.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Left}: RNN with h(green), \textbf  {Right}: LSTM with h(hidden vector, green) and c (cell state vector, yellow) }}{47}{figure.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces LSTM first equation diagram}}{47}{figure.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces LSTM diagram}}{48}{figure.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces First, decide what information we\IeC {\textquoteright }re going to throw away from the cell state}}{48}{figure.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Second, decide what information we\IeC {\textquoteright }re going to store in the cell state}}{49}{figure.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Third, update the old cell state}}{49}{figure.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Fourth, decide what we\IeC {\textquoteright }re going to output}}{50}{figure.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Variants on LSTMs}{50}{subsection.9.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces One popular LSTM variant, introduced by Gers \& Schmidhuber (2000), is adding ``peephole connections.\IeC {\textquotedblright } This means that we let the gate layers look at the cell state. The diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.}}{50}{figure.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we\IeC {\textquoteright }re going to input something in its place. We only input new values to the state when we forget something older.}}{50}{figure.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single ``update gate.\IeC {\textquotedblright } It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.}}{51}{figure.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Difference between RNN and LSTM}{51}{subsection.9.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Difference between RNN (\textbf  {left}) and LSTM (\textbf  {right})}}{51}{figure.16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}Bidirectional RNN}{52}{subsection.9.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7}Summary}{52}{subsection.9.7}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Attention Models}{52}{section.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Soft Attention for Captioning}{52}{subsection.10.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Soft Attention for Captioning - Xu et al, \IeC {\textquotedblleft }Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\IeC {\textquotedblright }, ICML 2015}}{53}{figure.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Soft vs Hard Attention}{53}{subsection.10.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Soft Attention vs Hard Attention}}{53}{figure.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Example Soft Attention vs Hard Attention}}{54}{figure.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Recap}{54}{subsection.10.3}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Generative Adversarial Nets (GAN)}{54}{section.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces GAN}}{55}{figure.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces GAN}}{55}{figure.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Examples}{55}{subsection.11.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Interpolating random noise vectors}}{55}{figure.22}}
\newlabel{fig:gans1}{{22}{55}{Interpolating random noise vectors}{figure.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Generator learning a useful representation of the data}}{56}{figure.23}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Reinforcement Learning Basic Model}{56}{section.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Basic Model of RL}}{56}{figure.24}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{56}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{57}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{57}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{Defintition}{57}{section*.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Information State}{57}{subsection.12.1}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{57}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Proposition}{57}{section*.14}}
\@writefile{toc}{\contentsline {subparagraph}{Proof}{57}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Observability}{57}{subsection.12.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Components of RL agent}{58}{subsection.12.3}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{58}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{58}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{58}{section*.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4}Learning and Planning}{58}{subsection.12.4}}
\@writefile{toc}{\contentsline {section}{\numberline {13}Markov Decision Process}{58}{section.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Introduction to MDP}{58}{subsection.13.1}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{59}{section*.19}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{59}{section*.20}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{59}{section*.21}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{59}{section*.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Markov Decision Process}{59}{subsection.13.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Value function of a Markov Reward Process}}{60}{figure.25}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{60}{section*.23}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{60}{section*.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces A Markov Decision Process}}{61}{figure.26}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{61}{section*.25}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{61}{section*.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Recursive relation of value functions}}{62}{figure.27}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{62}{section*.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Computing state-value function using the Bellman Expectation Equation}}{63}{figure.28}}
\@writefile{toc}{\contentsline {paragraph}{Theorem}{63}{section*.29}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{63}{section*.30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}Infinite MDPs}{63}{subsection.13.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Computing optimal action-value function using the Bellman Optimality Equation}}{64}{figure.29}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Finding the Optimal Policy}}{64}{figure.30}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Finding Optimal value function $v_*(s)$}}{65}{figure.31}}
